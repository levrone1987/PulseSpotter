{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f947fd92-597d-4e1a-8cab-1703ccf0d530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from bson import ObjectId\n",
    "from pymongo import MongoClient, errors\n",
    "from pymongo.database import Database\n",
    "from tqdm import tqdm\n",
    "\n",
    "# bertopic components\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import OpenAI\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ea115-2ca4-4611-8353-ad70c9dbffd4",
   "metadata": {},
   "source": [
    "# Data collection and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de19c9d-8fba-4a1a-bcd1-9f89d3f93c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE = 'insightfinder-dev'\n",
    "\n",
    "DOCS_COLLECTION = 'documents'\n",
    "DOC_EMBEDDINGS_COLLECTION = 'embeddings'\n",
    "TOPICS_COLLECTION = 'topics'\n",
    "TOPIC_EMBEDDINGS_COLLECTION = 'topic_embeddings'\n",
    "\n",
    "MONGO_HOST = os.getenv(\"MONGO_HOST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a0b61a-5c47-4f82-acd2-c5c1081d3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_transaction(func):\n",
    "    \"\"\"\n",
    "    A decorator to execute a MongoDB operation with error handling.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            with MongoClient(MONGO_HOST) as mongo_client:\n",
    "                db = mongo_client[DATABASE]\n",
    "                return func(db, *args, **kwargs)\n",
    "        except errors.PyMongoError as e:\n",
    "            print(f\"MongoDB error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "    return wrapper\n",
    "        \n",
    "\n",
    "@mongo_transaction\n",
    "def get_data(db: Database, start_date: str, end_date: str, limit: int = None):\n",
    "    \"\"\"\n",
    "    Retrieves the content and embedding data for all ingested articles.\n",
    "    \"\"\"\n",
    "    docs_collection = db[DOCS_COLLECTION]\n",
    "    embeddings_collection = db[DOC_EMBEDDINGS_COLLECTION]\n",
    "\n",
    "    # get embeddings\n",
    "    embeddings = list(\n",
    "        embeddings_collection.find(\n",
    "            filter={\"embedding\": {\"$exists\": True}},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # get documents with embeddings\n",
    "    doc_ids_with_embedding = [doc[\"_id\"] for doc in embeddings]\n",
    "    documents = docs_collection.find(\n",
    "        filter={\n",
    "            \"_id\": {\"$in\": doc_ids_with_embedding},\n",
    "            \"parsed_date\": {\"$gte\": start_date, \"$lte\": end_date},\n",
    "        },\n",
    "        projection={\n",
    "            \"_id\": 1, \"url\": 1, \"parsed_date\": 1,\n",
    "            \"title\": 1, \"description\": 1, \"paragraphs\": 1\n",
    "        }\n",
    "    )\n",
    "    if limit:\n",
    "        documents = documents.limit(limit)\n",
    "    documents = list(documents)\n",
    "\n",
    "    # filter embeddings\n",
    "    result_ids = {doc[\"_id\"] for doc in documents}\n",
    "    embeddings = [doc for doc in embeddings if doc[\"_id\"] in result_ids]\n",
    "    return documents, embeddings\n",
    "\n",
    "\n",
    "@mongo_transaction\n",
    "def check_topics_exist(db: Database, start_date: str, end_date: str):\n",
    "    \"\"\"\n",
    "    Checks if we already have topics calculated for (start_date, end_date) pair.\n",
    "    \"\"\"\n",
    "    topics_collection = db[TOPICS_COLLECTION]\n",
    "    query = {\"topic_start_date\": {\"$eq\": start_date}, \"topic_end_date\": {\"$eq\": end_date}}\n",
    "    return topics_collection.find_one(query) is not None\n",
    "\n",
    "\n",
    "@mongo_transaction\n",
    "def insert_topics(db: Database, topics_data: list):\n",
    "    \"\"\"\n",
    "    Insert topic data into the topics collection.\n",
    "    \"\"\"\n",
    "    topics_collection = db[TOPICS_COLLECTION]\n",
    "    topics_collection.insert_many(topics_data)\n",
    "\n",
    "\n",
    "@mongo_transaction\n",
    "def insert_topic_embeddings(db: Database, topics_to_embeddings: dict):\n",
    "    \"\"\"\n",
    "    Insert topic embeddings into the topic embeddings collection.\n",
    "    \"\"\"\n",
    "    topic_embeddings_collection = db[TOPIC_EMBEDDINGS_COLLECTION]\n",
    "    payload = [{\"_id\": topic_id, \"embedding\": topic_embedding} for topic_id, topic_embedding in topics_to_embeddings.items()]\n",
    "    topic_embeddings_collection.insert_many(payload)\n",
    "\n",
    "\n",
    "def extract_article_content(document: dict):\n",
    "    \"\"\"\n",
    "    Transforms articles content data (documents) by merging the title, description and paragraphs.\n",
    "    \"\"\"\n",
    "    title = document.get(\"title\")\n",
    "    if title:\n",
    "        title = title.strip()\n",
    "\n",
    "    description = document.get(\"description\")\n",
    "    if description:\n",
    "        description = description.strip()\n",
    "\n",
    "    paragraphs = document.get(\"paragraphs\")\n",
    "    if isinstance(paragraphs, list):\n",
    "        paragraphs = (\" \".join(p.strip() for p in paragraphs if len(p.strip()) > 0)).strip()\n",
    "        if description is not None and paragraphs.startswith(description):\n",
    "            description = None\n",
    "\n",
    "    article_items = [title or \"\", description or \"\", paragraphs or \"\"]\n",
    "    return \"\\n\".join([item for item in article_items if len(item) > 0])\n",
    "\n",
    "\n",
    "def build_topic_model(openai_model_key: str) -> BERTopic:\n",
    "    # GPT model is used to provide a name for each topic based on a set of topic examplar documents\n",
    "    openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    openai_model = OpenAI(\n",
    "        openai_client,\n",
    "        model=openai_model_key,\n",
    "        chat=True,\n",
    "        tokenizer=\"char\",\n",
    "        doc_length=1000,\n",
    "        nr_docs=5,\n",
    "    )\n",
    "    \n",
    "    # components of the BERTopic algorithm\n",
    "    umap_model = UMAP(n_neighbors=10, n_components=3, min_dist=0.0, metric=\"cosine\")\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=2)\n",
    "    ctfidf_model = ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)\n",
    "    vectorizer_model = CountVectorizer(min_df=10, ngram_range=(1, 3))\n",
    "    \n",
    "    return BERTopic(\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        ctfidf_model=ctfidf_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        # representation_model=openai_model,\n",
    "        # verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d331c20-9315-4b7d-8c75-ed2491ca4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "start_date = \"2024-07-27\"\n",
    "end_date = \"2024-08-05\"\n",
    "\n",
    "openai_model_key = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32df2970-3a4e-4ed7-abaa-94fd1ca9c31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2024-07-29', '2024-08-05'], dtype='datetime64[ns]', freq='W-MON')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mondays = pd.date_range(start_date, end_date, freq=\"W-MON\")\n",
    "mondays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d92677d-007c-49a3-9ca8-34cda8c0532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating topics: 100%|████████████████████████| 1/1 [04:13<00:00, 253.97s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pbar = tqdm(\n",
    "    zip(mondays, mondays[1:]),\n",
    "    total=len(mondays) - 1,\n",
    "    desc=\"Calculating topics\",\n",
    "    leave=True,\n",
    "    position=0,\n",
    ")\n",
    "\n",
    "for monday, next_monday in pbar:\n",
    "    sunday = next_monday - timedelta(days=1)\n",
    "    monday_str = monday.strftime(\"%Y-%m-%d\")\n",
    "    sunday_str = sunday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # skip already ingested topics\n",
    "    if check_topics_exist(monday_str, sunday_str):\n",
    "        print(f\"Skipping topics creation for {monday_str} - {sunday_str} ...\")\n",
    "        continue\n",
    "\n",
    "    # extract documents and embeddings within the date range\n",
    "    documents, embeddings = get_data(monday.strftime(\"%Y-%m-%d\"), sunday.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # align order of documents and embeddings\n",
    "    doc_order = [doc[\"_id\"] for doc in embeddings]\n",
    "    documents = sorted(documents, key=lambda doc: doc_order.index(doc[\"_id\"]))\n",
    "    assert [doc[\"_id\"] for doc in documents] == [doc[\"_id\"] for doc in embeddings]\n",
    "\n",
    "    # extract the article contents and embedding vectors into separate lists\n",
    "    articles = [extract_article_content(doc) for doc in documents]\n",
    "    vectors = [doc[\"embedding\"] for doc in embeddings]\n",
    "\n",
    "    # build topic model\n",
    "    topic_model = build_topic_model(openai_model_key)\n",
    "\n",
    "    # calculate topic assignments and assignment probabilities (of document to topic)\n",
    "    topics_assignment, assignment_probs = topic_model.fit_transform(\n",
    "        documents=articles,\n",
    "        embeddings=np.array(vectors),\n",
    "    )\n",
    "    topics_labels = topic_model.topic_labels_\n",
    "    topic_embeddings = topic_model.topic_embeddings_\n",
    "\n",
    "    # generate unique identifiers for computed topics\n",
    "    topic_ids = {topic_idx: ObjectId() for topic_idx in topics_labels}\n",
    "\n",
    "    # insert topic data into the db\n",
    "    topics_data = []\n",
    "    for i in range(len(documents)):\n",
    "        document = documents[i]\n",
    "        topic_index = topics_assignment[i]\n",
    "        topic_label = topics_labels[topic_index]\n",
    "        topics_data.append({\n",
    "            \"document_id\": document[\"_id\"],\n",
    "            \"document_date\": document[\"parsed_date\"],\n",
    "            \"topic_index\": topic_index,\n",
    "            \"topic_label\": topic_label,\n",
    "            \"assignment_probability\": assignment_probs[i],\n",
    "            \"topic_start_date\": monday.strftime(\"%Y-%m-%d\"),\n",
    "            \"topic_end_date\": sunday.strftime(\"%Y-%m-%d\"),\n",
    "            \"topic_id\": topic_ids[topic_index],\n",
    "        })\n",
    "    insert_topics(topics_data)\n",
    "\n",
    "    # insert topic embeddings into the db\n",
    "    topic_to_embedding = {}\n",
    "    for topic_index in topics_labels:\n",
    "        topic_id = topic_ids[topic_index]\n",
    "        topic_embedding = topic_embeddings[topic_index]\n",
    "        topic_to_embedding[topic_id] = list(topic_embedding)\n",
    "    insert_topic_embeddings(topic_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd2ee51-b590-4c29-a474-a2eb091dde01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip topics creation for 2024-07-29 - 2024-08-04? True\n"
     ]
    }
   ],
   "source": [
    "# checking if we manage to prevent overwriting the precalcuated topics\n",
    "\n",
    "mondays = pd.date_range(start_date, end_date, freq=\"W-MON\")\n",
    "\n",
    "for monday, next_monday in zip(mondays, mondays[1:]):\n",
    "    sunday = next_monday - timedelta(days=1)\n",
    "    monday_str = monday.strftime(\"%Y-%m-%d\")\n",
    "    sunday_str = sunday.strftime(\"%Y-%m-%d\")\n",
    "    print(f\"Skip topics creation for {monday_str} - {sunday_str}?\", check_topics_exist(monday_str, sunday_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e487d7c-2bcc-4295-a6f0-39d2817362a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insightfinder",
   "language": "python",
   "name": "insightfinder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
